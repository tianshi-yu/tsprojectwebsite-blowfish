---
title: "Data-driven Modelling for Human–Robot Interaction in the Mining and Maritime Industries"
date: 2024-07-01
description: "Research Project"
summary: "Developed human-state prediction models for VR training that supports effective human–autonomy collaboration."
tags: ["VR (Unreal)", "EMG", "Motion" , "Eye Gaze", "Transformer"]
showDate: false
---
{{< katex >}}

## VR Platform for Studying Human–Autonomy Interaction

- Developed a VR platform in Unreal Engine (C++) that simulates a human-operated vehicle interacting with autonomous vehicles. [[Demo Code](https://github.com/tianshi-yu/HRVREP)]

- Developed a multi-threaded sensor data acquisition and logging system with software interfaces for the **VIVE Eye Tracker**, **Xsens** motion-capture system, and a standalone Unreal plugin for **Delsys EMG**. [[Plugin Code](https://github.com/tianshi-yu/HRVREP/tree/main/Plugins/DelsysTrignoEMG)]

{{< youtubeLite id="EV_sVHGsjKo" label="Unreal demo" >}}

---

## Data-driven Model for Human-State Prediction

- Developed machine-learning models to predict human attention (gaze) and affective states (stress, trust, workload) using **Transformer** architectures in **PyTorch**, supporting human-centred adaptive VR training solutions for the mining and maritime industries.


